\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Problem Definition}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Keyword Extraction Methodology}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Keyword Extraction}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Algorithms}{6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Extract Keyword Noun Phrase from a Sentence}}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Dataset Description}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Performance Analysis}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Keyword Extraction}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Bar plot of f-measure, precision, and recall values computed for RAKE, LDA, NMF, and Cosine Similarity Minimization respectively.}}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Classification Performance}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Bar plot of f-measure values obtained with frequency based features and a SVM, as well as models trained with our proposed embedding based features. The multilayer perceptron achieved the highest performance out of all the models trained with our proposed features.}}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Communities of Practice: Learning to Classify Publication Data}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Convolutional Neural Network for Text Classification}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Abstract text is mapped to a padded sequence of word indexes.}}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Example of a network's input. The network receives as input a sequence of word indexes i.e. integers maps that sequence to a sequence of word embeddings, one per word index.}}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces After applying the embedding mapping operation, we convolve the data with filter to create 'feature maps'. We use sliding receptive fields to capture context within the text.}}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Convolutional Neural Network for Text Classification}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces We apply convolution operations, sliding filters vertically to capture word context.}}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}ACM Abstracts: Communities of Practice Data}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Class distribution of ACM abstract dataset.}}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}NSF Research Award Abstracts Data}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Class distribution of NSF abstract dataset.}}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Performance plot on validation set. Vocabulary size is 30000 words, sequence length is 1000.}}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Performance plot on validation set. Vocabulary size is 30000 words, sequence length is 1000.}}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Performance plot on validation set. Vocabulary size is 20000 words, sequence length is 250.}}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Loss plot on validation set. Vocabulary size is 30000 words, sequence length is 1000.}}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Loss plot on validation set. Vocabulary size is 10000 words, sequence length is 500.}}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Loss plot on validation set. Vocabulary size is 5000 words, sequence length is 500.}}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Loss plot. Vocabulary size is 20000 words, sequence length is 250.}}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Arxiv Abstracts Data}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Class distribution in the Arxiv abstract dataset. There are 40 computer science related topics, with up to 5000 publications in a topic.}}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Performance plot on Arxiv data. Vocabulary size is 20000 words, sequence length is 150.}}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Loss plot on Arxiv data. Vocabulary size is 20000 words, sequence length is 150.}}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {6}CSM Features and Layer}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Data Augmentation}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Example of data augmentation. We apply random permutations of the word indexes on windows of size 2 to 5, randomly chosen at the beginning of each training epoch. Here a window of 4 is applied.}}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{20}}
